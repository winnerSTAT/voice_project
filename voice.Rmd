---
title: "Term Project: Gender Recognition by Voice"
author: "G2Group05 CHEN XIANG, XU YUWEN, YANG YUBO, ZHANG YUN"
date: "11/17/2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Note:

This is the R Markdown document for term project. In this file, we detailly included all the codes that we used to analysis the voice data, for predicting the gender of the voice. This is only the technical part, the term report chould be found as another independent file.

The training and testing data are downloaded from Kaggle, see <https://www.kaggle.com/primaryobjects/voicegender>. We also generated our own voice, transfered them to data in the same format, and then did our validation.


## Data Loading and Preperation
### Data explanation and screening
The following acoustic properties of each voice are measured and included within the dataset:

- "meanfreq": mean frequency (in kHz)
- "sd": standard deviation of frequency
- "median": median frequency (in kHz)
- "Q25": first quantile (in kHz)
- "Q75": third quantile (in kHz)
- "IQR": interquantile range (in kHz)
- "skew": skewness (see note in specprop description)
- "kurt": kurtosis (see note in specprop description)
- "sp.ent": spectral entropy
- "sfm": spectral flatness
- "mode": mode frequency
- "centroid": frequency centroid (see specprop)
- "peakf": peak frequency (frequency with highest energy)
- "meanfun": average of fundamental frequency measured across acoustic signal
- "minfun": minimum fundamental frequency measured across acoustic signal
- "maxfun": maximum fundamental frequency measured across acoustic signal
- "meandom": average of dominant frequency measured across acoustic signal
- "mindom": minimum of dominant frequency measured across acoustic signal
- "maxdom": maximum of dominant frequency measured across acoustic signal
- "dfrange": range of dominant frequency measured across acoustic signal
- "modindx": modulation index. Calculated as the accumulated absolute difference between adjacent measurements of fundamental frequencies divided by the frequency range
- "label": male or female

```{r}
voice <- read.csv("~/Desktop/voice.csv")
summary(voice)
```


### Preperation of training and testing dataset.

```{r}
set.seed(123)
# randomly set the training data and test dataset, keeping balance in gender
test.index <- c(sample(1584,150), sample(1585:3168,150))
train.index <- -test.index
x <- voice[ ,-21]
y <- voice[ ,21]
x.train <- x[train.index, ]
x.test <- x[test.index, ]
y.train <- y[train.index]
y.test <- y[test.index]
```


## Data Effeciency Validation
We perform a kmeans clustering analysis, based on principle component analysis (pca).

* First of all, we used clustering analysis to validate whether the data collected is effecient for the analysis, by looking at x-columns and check whether they can be classified into two differentiated groups.

* We used pca first to reduce dimension of the dataset, then choosed 4 most important PCs (75% sdev) to perform the kmeans clustering.

### Step.1 processing PCA for voice data
```{r}
x.pca <- prcomp(x, scale=TRUE)
# loadings of each PC
head(x.pca$rotation)
plot(x.pca)

# proportion of variance explained
x.pca$sdev
plot(x.pca$sdev^2 / sum(x.pca$sdev^2), type="b", ylim=c(0,1))

# cumulative proportion of variance explained
plot(cumsum(x.pca$sdev^2) / sum(x.pca$sdev^2), type="b", ylim=c(0,1))
abline(h=0.7, lty = 2)

# to ensure that the pca explains 70+% of the standard deviation, we choose first 4 componets
sum(data.frame(x.pca$sdev^2)[1:4,]) / sum(x.pca$sdev^2) 

PC <- data.frame(predict(x.pca, x)[,1:4])
PC.train <- PC[train.index, ]
PC.test <- PC[test.index, ]
```

Note that the PC data frame contains the first 4 PCs. It will be used as the predictors in the following kmeans algorithom.

### Step.2 kmeans clustering modelling with training data

Firstly, we use the training data to from the kmeans model. 
```{r}
km <- kmeans(x=PC.train, centers = 2)
summary(km)
train.cluster <- as.factor(ifelse(km$cluster ==1, "female","male"))
mat <- table(train.cluster, y.train)
mat
accuracy.kmeans.train <- (mat[1,1]+mat[2,2])/sum(mat)
accuracy.kmeans.train
```
So the in sample prediction accuracy is **0.6433054**. 

We plot the prediction and the clustering centers.

```{r}
library("ggplot2")
ggplot(data=PC.train, aes(x=PC1, y=PC2)) + geom_point(aes(color=factor(train.cluster)), size=4) + theme_bw() + geom_point(data=as.data.frame(km$centers), size=6)
ggplot(data=PC.train, aes(x=PC3, y=PC4)) + geom_point(aes(color=factor(train.cluster)), size=4) + theme_bw() + geom_point(data=as.data.frame(km$centers), size=6)
```
From the plot, we know the clustering is mainly according to **PC1**.

We can also plot the predicted cluster together with the true cluster.
```{r}
ggplot(data=PC.train, aes(x=PC1, y=PC2)) + geom_point(aes(color=train.cluster), size=4) + geom_point(aes(color=factor(y.train)), size=2) + theme_bw() 
ggplot(data=PC.train, aes(x=PC3, y=PC4)) + geom_point(aes(color=train.cluster), size=4) + geom_point(aes(color=factor(y.train)), size=2) + theme_bw() 
```

### Step.3 model validation with test data

We use the model to predict for the testing data. Then compare with the prediction accuracy.

```{r}
library("clue")
km_pred <- cl_predict(km, PC.test)
test.cluster <- as.factor(ifelse(km_pred ==1, "female","male"))
mat.test <- table(test.cluster, y.test)
mat.test
accuracy.kmeans.test <- (mat.test[1,1]+mat.test[2,2])/sum(mat.test)
accuracy.kmeans.test
```
The accuracy in testing data is **0.6233333**.

Plot the prediction and the clustering centers
```{r}
ggplot(data=PC.test, aes(x=PC1, y=PC2)) + geom_point(aes(color=factor(test.cluster)), size=4) + theme_bw() + geom_point(data=as.data.frame(km$centers), size=6)
ggplot(data=PC.test, aes(x=PC3, y=PC4)) + geom_point(aes(color=factor(test.cluster)), size=4) + theme_bw() + geom_point(data=as.data.frame(km$centers), size=6)
```

Plot the predicted cluster together with the true cluster.
```{r}
ggplot(data=PC.test, aes(x=PC1, y=PC2)) + geom_point(aes(color=test.cluster), size=4) + geom_point(aes(color=factor(y.test)), size=2) + theme_bw() 
ggplot(data=PC.test, aes(x=PC3, y=PC4)) + geom_point(aes(color=test.cluster), size=4) + geom_point(aes(color=factor(y.test)), size=2) + theme_bw() 
```

The conclusion is that, after pca, the acoustic data is actually differenciated with out the label. The dataset should be effecient for prediction.

## Logistic Regression
```{r}
# build a naive logistic regression model
voice.glm <- glm(y.train ~ ., data=x.train, family=binomial(link="logit"))
summary(voice.glm)
# predictor selection
library("MASS")
glm.best <- stepAIC(voice.glm, direction="backward")
summary(glm.best)
```

Then we need to analyze the performance of the model using the test data set.

```{r}
library("ROCR")
prob.glm <- predict(glm.best, newdata=x.test, type="response")
prediction.glm <- prediction(prob.glm, y.test)
ROC.glm <- performance(prediction.glm, measure = "tpr", x.measure = "fpr")
plot(ROC.glm, col="green", main = "logistic regression ROC")

# the AUC
auc.glm <- as.numeric(performance(prediction.glm, "auc")@y.values)
auc.glm

# plot the cutoff with the error rate and find the optimal cutoff
err.glm <- performance(prediction.glm, measure = "err")
plot(err.glm, col="green", main = "logistic regression")
optimal.glm <- which.min(err.glm@y.values[[1]])
cutoff.glm <- err.glm@x.values[[1]][which.min(err.glm@y.values[[1]])]

# prediction with the cutoff
pred.glm <- (prob.glm > cutoff.glm)
table.glm <- table(pred.glm, y.test)
table.glm
accuracy.glm <- (table.glm[1, 1] + table.glm[2,2])/sum(table.glm)
accuracy.glm
```
Logistic regression model is performing **0.9733333** accuracy in testing data.

## Regularization in Linear Models
### 1. Ridge Model
```{r}
library("glmnet")
x.train.mat <- model.matrix(label ~ ., voice[train.index, ])[, -1]
x.test.mat <- model.matrix(label ~ ., voice[test.index, ])[, -1]

# cross-validation to find the optimal lambda
set.seed(123)
ridge.cv <- cv.glmnet(x.train.mat, y.train, alpha=0, family="binomial", type.measure="class") 
plot(ridge.cv)

# optimal lambda
ridge.lam <- ridge.cv$lambda.1se  # or ridge.cv$lambda.min

# plot optimal model
ridge.mod <- glmnet(x.train.mat, y.train, alpha=0, family="binomial")
plot(ridge.mod, xvar="lambda", label = TRUE)
abline(v=log(ridge.lam), lty=2)
```

Now we use the test data to do the validation of how the model performaces.

```{r}
prob.ridge <- predict(ridge.mod, newx=x.test.mat, s=ridge.lam, type="response", exact=TRUE)
prediction.ridge <- prediction(prob.ridge, y.test)
ROC.ridge <- performance(prediction.ridge, measure = "tpr", x.measure = "fpr")
plot(ROC.ridge, col="blue", main = "Ridge regression ROC")

# the AUC
auc.ridge <- as.numeric(performance(prediction.ridge, "auc")@y.values)
auc.ridge

# plot the cutoff with the error rate and find the optimal cutoff
err.ridge <- performance(prediction.ridge, measure = "err")
plot(err.ridge, col="blue", main = "Ridge regression")
optimal.ridge <- which.min(err.ridge@y.values[[1]])
cutoff.ridge <- err.ridge@x.values[[1]][which.min(err.ridge@y.values[[1]])]

# prediction with the cutoff
pred.ridge <- (prob.ridge > cutoff.ridge)
table.ridge <- table(pred.ridge, y.test)
table.ridge
accuracy.ridge <- (table.ridge[1, 1] + table.ridge[2,2])/sum(table.ridge)
accuracy.ridge
```
Ridge regression model is performing **0.9633333** accuracy in testing data.

### 2.LASSO model
```{r}
# cross-validation
set.seed(123)
lasso.cv <- cv.glmnet(x.train.mat, y.train, alpha=1, family="binomial", type.measure="class")
plot(lasso.cv)

# optimal lambda
lasso.lam <- lasso.cv$lambda.1se  # lasso.cv$lambda.min

# plot optimal model
lasso.mod <- glmnet(x.train.mat, y.train, alpha=1, family="binomial")
plot(lasso.mod, xvar="lambda", label = TRUE)
abline(v=log(lasso.lam), lty=2)
```

We also use the test data to measure the performance.

```{r}
prob.lasso <- predict(lasso.mod, newx=x.test.mat, s=lasso.lam, type="response", exact=TRUE)
prediction.lasso <- prediction(prob.lasso, y.test)
ROC.lasso <- performance(prediction.lasso, measure = "tpr", x.measure = "fpr")
plot(ROC.lasso, col="purple", main = "LASSO regression ROC")

# the AUC
auc.lasso <- as.numeric(performance(prediction.lasso, "auc")@y.values)
auc.lasso

# plot the cutoff with the error rate and find the optimal cutoff
err.lasso <- performance(prediction.lasso, measure = "err")
plot(err.lasso, col="purple", main = "LASSO regression")
optimal.lasso <- which.min(err.lasso@y.values[[1]])
cutoff.lasso <- err.lasso@x.values[[1]][which.min(err.lasso@y.values[[1]])]

# prediction with the cutoff
pred.lasso <- (prob.lasso > cutoff.lasso)
table.lasso <- table(pred.lasso, y.test)
table.lasso
accuracy.lasso <- (table.lasso[1, 1] + table.lasso[2,2])/sum(table.lasso)
accuracy.lasso
```
LASSO regression model is performing **0.97** accuracy in testing data.

### 3. Elastic Net Model
```{r}
alphas <- seq(0, 1, 0.05)
K <- 10
n <- nrow(x.train.mat)
fold <- rep(0, n)
set.seed(123)
shuffled.index <- sample(n, n, replace=FALSE)
fold[shuffled.index] <- rep(1:K, length.out=n)
table(fold)

en.cv.error <- data.frame(alpha=alphas)
for (i in 1:length(alphas)){
  en.cv <- cv.glmnet(x.train.mat, y.train, alpha=alphas[i], foldid=fold, family="binomial", type.measure="class")
  en.cv.error[i, "lambda"] <- en.cv$lambda.1se
  en.cv.error[i, "error"] <- min(en.cv$cvm) + en.cv$cvsd[which.min(en.cv$cvm)]
}

# optimal lambda and alpha
en.lam <- en.cv.error[which.min(en.cv.error$error), "lambda"]
en.alpha <- en.cv.error[which.min(en.cv.error$error), "alpha"]

# plot optimal alpha
plot(en.cv.error$alpha, en.cv.error$error, type="l")
abline(v=en.alpha, lty=2)

# plot the optimal model
en.mod <- glmnet(x.train.mat, y.train, alpha=en.alpha, family="binomial")
plot(en.mod, xvar="lambda", label = TRUE)
abline(v=log(en.lam), lty=2)
```

The testing performance again.

```{r}
prob.en <- predict(en.mod, newx=x.test.mat, s=en.lam, type="response", exact=TRUE)
prediction.en <- prediction(prob.en, y.test)
ROC.en <- performance(prediction.en, measure = "tpr", x.measure = "fpr")
plot(ROC.en, col="black", main = "EN regression ROC")

# the AUC
auc.en <- as.numeric(performance(prediction.en, "auc")@y.values)
auc.en

# plot the cutoff with the error rate and find the optimal cutoff
err.en <- performance(prediction.en, measure = "err")
plot(err.en, col="black", main = "EN regression")
optimal.en <- which.min(err.en@y.values[[1]])
cutoff.en <- err.en@x.values[[1]][which.min(err.en@y.values[[1]])]

# prediction with the cutoff
pred.en <- (prob.en > cutoff.en)
table.en <- table(pred.en, y.test)
table.en
accuracy.en <- (table.en[1, 1] + table.en[2,2])/sum(table.en)
accuracy.en
```
The testing performance of en regiression is **0.9666667**.

## Tree
```{r}
library("tree")
#grow a tree
voice.tree <- tree(y.train ~ ., data=x.train)

#pruning by cross-validation
set.seed(123)
voice.tree.cv <- cv.tree(voice.tree, method="misclass")

# optimal tree size obtained by CV
optimal <- which.min(voice.tree.cv$dev)
optimal.size <- voice.tree.cv$size[optimal]
optimal
optimal.size

# pruned tree
voice.tree.pruned <- prune.tree(voice.tree, best=optimal.size, method="misclass")
plot(voice.tree.pruned)
text(voice.tree.pruned)
```

Test data prediction.

```{r}
# prediction on test data
prob.tree <- predict(voice.tree.pruned, newdata=x.test, type="vector")[, 2]
prediction.tree <- prediction(prob.tree, y.test)
ROC.tree <- performance(prediction.tree, measure = "tpr", x.measure = "fpr")
plot(ROC.tree, col="red", main = "Tree ROC")

# the AUC
auc.tree <- as.numeric(performance(prediction.tree, "auc")@y.values)
auc.tree

# plot the cutoff with the error rate and find the optimal cutoff
err.tree <- performance(prediction.tree, measure = "err")
plot(err.tree, col="red", main = "Tree")
optimal.tree <- which.min(err.tree@y.values[[1]])
cutoff.tree <- err.tree@x.values[[1]][which.min(err.tree@y.values[[1]])]

# prediction with the cutoff
pred.tree <- (prob.tree > cutoff.tree)
table.tree <- table(pred.tree, y.test)
table.tree
accuracy.tree <- (table.tree[1, 1] + table.tree[2,2])/sum(table.tree)
accuracy.tree
```
The baseline tree model prediction accuracy is **0.91**.


## RandomForest
```{r}
library("randomForest")
# tune random forest (mtry) manually
mse.rfs <- rep(0, 20)
set.seed(123)
for(m in 1:20){
  rf <- randomForest(y.train ~ ., data=x.train, ntree=501, mtry=m)
  mse.rfs[m] <- rf$err.rate[501]
}
plot(1:20, mse.rfs, type="b", xlab="mtry", ylab="OOB Error")
m.optimal <- which.min(mse.rfs)
m.optimal

# fit a random forest model
set.seed(123)
voice.rf <- randomForest(y.train ~ ., data=x.train,  ntree=501, mtry=m.optimal, importance=TRUE)
plot(voice.rf)

# variable importance
varImpPlot(voice.rf)

# partial plot in RF
partialPlot(voice.rf, voice[train.index, ], x.var="meanfun", which.class="male")
partialPlot(voice.rf, voice[train.index, ], x.var="IQR", which.class="male")
```

Test data performance.

```{r}
prob.rf <- predict(voice.rf, newdata=x.test, type="prob")[, 2]
prediction.rf <- prediction(prob.rf, y.test)
ROC.rf <- performance(prediction.rf, measure = "tpr", x.measure = "fpr")
plot(ROC.rf, col="orange", main = "RandomForest ROC")

# the AUC
auc.rf <- as.numeric(performance(prediction.rf, "auc")@y.values)
auc.rf

# plot the cutoff with the error rate and find the optimal cutoff
err.rf <- performance(prediction.rf, measure = "err")
plot(err.rf, col="orange", main = "RandomForest regression")
optimal.rf <- which.min(err.rf@y.values[[1]])
cutoff.rf <- err.rf@x.values[[1]][which.min(err.rf@y.values[[1]])]

# prediction with the cutoff
pred.rf <- (prob.rf > cutoff.rf)
table.rf <- table(pred.rf, y.test)
table.rf
accuracy.rf <- (table.rf[1, 1] + table.rf[2,2])/sum(table.rf)
accuracy.rf
```
The prediction of Random Forest is **0.9833333**.

## GBM
```{r, warning=FALSE, message=FALSE}
library("gbm")
y.gbm <- as.numeric(voice[, "label"]) - 1
y.gbm.train <- y.gbm[train.index]

# tune the prameter ntree
set.seed(321)
tune.out <- data.frame()
lambda <- 0.01
d <-4
ns <- (1:5) * 50 / (lambda * sqrt(4))
for (i in 1:length(ns) )
{
  gbm.mod <- gbm(y.gbm.train ~ ., data=x.train, distribution="bernoulli", n.trees=ns[i], interaction.depth=d, shrinkage=lambda, cv.folds=10)
  n.opt <- gbm.perf(gbm.mod)
  pred.gbm <- predict(gbm.mod, newdata=x.test, n.trees=n.opt, type="response") >0.5
  gbm.table <- table(pred.gbm, y.test)
  gbm.accuracy <- (gbm.table[1, 2] + gbm.table[2,1])/sum(gbm.table)
  out<- data.frame(n=ns[i],misrate=gbm.accuracy,n.opt=n.opt)
  tune.out <- rbind(tune.out,out)
}
tune.out
n.opt<- tune.out[which.min(tune.out$misrate),3]
gbm.mod
```

Test data performance.

```{r}
prob.gbm <- predict(gbm.mod, newdata=x.test, n.trees=n.opt, type="response")
prediction.gbm <- prediction(prob.gbm, y.test)
ROC.gbm <- performance(prediction.gbm, measure = "tpr", x.measure = "fpr")
plot(ROC.gbm, col="yellow", main = "gbm regression ROC")

# the AUC
auc.gbm <- as.numeric(performance(prediction.gbm, "auc")@y.values)
auc.gbm

# plot the cutoff with the error rate and find the optimal cutoff
err.gbm <- performance(prediction.gbm, measure = "err")
plot(err.gbm, col="yellow", main = "gbm regression")
optimal.gbm <- which.min(err.gbm@y.values[[1]])
cutoff.gbm <- err.gbm@x.values[[1]][which.min(err.gbm@y.values[[1]])]

# prediction with the cutoff
pred.gbm <- (prob.gbm > cutoff.gbm)
table.gbm <- table(pred.gbm, y.test)
table.gbm
accuracy.gbm <- (table.gbm[1, 1] + table.gbm[2,2])/sum(table.gbm)
accuracy.gbm
```
The prediction accuracy for gbm is **??**.


## SVM
Supportive Vector Machine (SVM) is a effective algorithom for prediction.

### 1. naive SVM modelling without tuning
```{r}
library("e1071")
svm.mod <- svm(label~., data=voice[train.index, ])
summary(svm.mod)
```

Test data performance:
```{r}
y_hat <- predict(svm.mod, voice[test.index,-21])

svm.mat <- table(y_hat,y.test)
svm.mat
accuracy.svm <- (svm.mat[1,1]+svm.mat[2,2])/sum(svm.mat)
accuracy.svm
```
The testing prediction accuracy is **0.9733333**.

### 2. tuning the pramaters *epsillon* and *cost*
```{r}
# perform a grid search
tuneResult <- tune(svm, label ~ .,  data = voice[train.index, ], ranges = list(epsilon = seq(0,1,0.1), cost = 2^(2:9)))

# Draw the tuning graph
plot(tuneResult)
# find the best performance ones
print(tuneResult)
tunedModel <- tuneResult$best.model
```

Test data performance:
```{r}
## test data performance
y_best_hat <- predict(tunedModel, voice[test.index,-21]) 

svm.best <- table(y_best_hat,y.test)
svm.best
accuracy.svm.best <- (svm.best[1,1]+svm.best[2,2])/sum(svm.best)
accuracy.svm.best
```
test accuracy is **0.9766667**, which is a little bit better than the naive svm model.

## Ensembled and Comparison
In order to generate an ensembled model so that a better prediction can be performed, we firstly do a quick summary of the performance of the supervised learning models that we already get till now, then we select the top 3 testing performed models to produce the ensembled modelling.

The ensemble algorithom that we used is the RandomForest based on the predicted probablity data for each row in the data set.

```{r}
accuracy <- data.frame(model=c("glm","ridge","lasso","en","tree","randomForest","gbm","svm"), acc=0, auc=0)

accuracy[which(accuracy$model=="glm"), 2:3] <- c(accuracy.glm, auc.glm)
accuracy[which(accuracy$model=="ridge"), 2:3] <- c(accuracy.ridge, auc.ridge)
accuracy[which(accuracy$model=="lasso"), 2:3] <- c(accuracy.lasso, auc.lasso)
accuracy[which(accuracy$model=="en"), 2:3] <- c(accuracy.en, auc.en)
accuracy[which(accuracy$model=="tree"), 2:3] <- c(accuracy.tree, auc.tree)
accuracy[which(accuracy$model=="randomForest"), 2:3] <- c(accuracy.rf, auc.rf)
accuracy[which(accuracy$model=="gbm"), 2:3] <- c(accuracy.gbm, auc.gbm)
accuracy[which(accuracy$model=="svm"), 2] <- c(accuracy.svm.best)

# display and find the most effective 3 models
accuracy[order(accuracy$acc, decreasing = TRUE),]
```
From the table above, we can tell that the best models are **randomForest**, **SVM** and **glm**.

Next we process to do the ensembling based on randomForest.
```{r}
ensem <- data.frame(prob.rf,y_best_hat,prob.glm,y.test)
ensem.mod <- tuneRF(ensem[,-4], ensem[,4], stepFactor=0.5, doBest=TRUE)
pred.ensem <- predict(ensem.mod, newdata=ensem)
mat.ensem <- table(pred.ensem,y.test)
mat.ensem
accuracy.ensem <- (mat.ensem[1,1]+mat.ensem[2,2])/sum(mat.ensem)
accuracy.ensem
```
The prediction accuracy of the ensemble model is **0.9866667** for the ensemble model, defeated all the models discussed earlier.

## Practical Testing: Using our own voice
In this part, we are going to include the codes we used to read a *wav* file into R workspace and extract the acoustic data into the same format with the data we used. Then with such tech, we are able to complete the whole process which is from collecting the data to the gender prediction.

```{r, eval=FALSE}
library("tuneR")
library("seewave")
library("fftw")

specan<-function(start,end,rec,bp=c(0,22),wl=2048){
  x<-(mapply(function(start,end,rec) { 
  r<-readWave(paste(rec),from = start,to=end,units = "seconds")
  songspec<-spec(r,f=r@samp.rate,plot=FALSE)
  b<- bp #in case bp its higher than can be due to sampling rate
  if(b[2] > ceiling(r@samp.rate/2000) - 1) b[2] <- ceiling(r@samp.rate/2000) - 1 
  
  #frequency spectrum analysis
  analysis<-specprop(songspec, f=r@samp.rate, flim=c(0, 280/1000), plot=FALSE)
  
  #save parameters
  meanfreq <- analysis$mean/1000
  sd <- analysis$sd/1000
  median <- analysis$median/1000
  Q25 <- analysis$Q25/1000
  Q75 <- analysis$Q75/1000
  IQR <- analysis$IQR/1000
  skew <- analysis$skewness
  kurt <- analysis$kurtosis
  sp.ent <- analysis$sh
  sfm <- analysis$sfm
  mode <- analysis$mode/1000
  centroid <- analysis$cent/1000
 
   #Frequency with amplitude peaks
  peakf<-0
  
  #Fundamental frequency parameters
  ff <- seewave::fund(r, f = r@samp.rate, ovlp = 50, threshold = 5, fmax = 280, ylim=c(0, 280/1000), plot = FALSE, wl = wl)[, 2]
  meanfun<-mean(ff, na.rm = T)
  minfun<-min(ff, na.rm = T)
  maxfun<-max(ff, na.rm = T)
  
  #Dominant frecuency parameters
  y <- seewave::dfreq(r, f = r@samp.rate, wl = wl, ylim=c(0, 280/1000), ovlp = 0, plot = F, threshold = 5, bandpass = b * 1000, fftw = TRUE)[, 2]
  meandom <- mean(y, na.rm = TRUE)
  mindom <- min(y, na.rm = TRUE)
  maxdom <- max(y, na.rm = TRUE)
  dfrange <- (maxdom - mindom)
  duration <- (end - start)
  
  #modulation index calculation
  changes <- vector()
  for(j in which(!is.na(y))){
    change <- abs(y[j] - y[j + 1])
    changes <- append(changes, change)
  }
  if(mindom==maxdom) modindx<-0 else modindx <- mean(changes, na.rm = T)/dfrange
return(cbind(duration, meanfreq, sd, median, Q25, Q75, IQR, skew, kurt, sp.ent, sfm, mode, centroid, peakf, meanfun, minfun, maxfun, meandom, mindom, maxdom, dfrange, modindx))}
  ,start=start,end=end,rec=rec))
  rownames(x)<-c("duration", "meanfreq", "sd", "median", "Q25", "Q75", "IQR", "skew", "kurt", "sp.ent", 
                 "sfm","mode", "centroid", "peakf", "meanfun", "minfun", "maxfun", "meandom", "mindom", "maxdom", "dfrange", "modindx")
  colnames(x)<-paste(rec,sep="-")
  return(x)}
```


## Session Info

```{r session-info}
print(sessionInfo(), locale=FALSE)
```

